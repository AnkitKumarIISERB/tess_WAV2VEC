# -*- coding: utf-8 -*-
"""tess_WAV2VEC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x_4KEVxT1iIffqh3MfJ_6uhryRyw8eyK
"""

# ===========================================
# 1Ô∏è‚É£ INSTALL LIBRARIES
# ===========================================
!pip install -q transformers datasets torchaudio librosa soundfile scikit-learn kaggle

# ===========================================
# 2Ô∏è‚É£ IMPORTS
# ===========================================
import os
import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from datasets import Dataset
from transformers import (
    Wav2Vec2Processor,
    Wav2Vec2ForSequenceClassification,
    Trainer,
    TrainingArguments
)

# ===========================================
# 3Ô∏è‚É£ UPLOAD kaggle.json
# ===========================================
from google.colab import files
files.upload()   # Upload your kaggle.json file here

# ===========================================
# 4Ô∏è‚É£ CONFIGURE KAGGLE
# ===========================================
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# ===========================================
# 5Ô∏è‚É£ DOWNLOAD & EXTRACT TESS DATASET
# ===========================================
!kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess
!unzip -q toronto-emotional-speech-set-tess.zip -d /content/TESS
data_dir = "/content/TESS"

# ===========================================
# 6Ô∏è‚É£ PREPARE FILES AND LABELS
# ===========================================
emotion_labels = ["angry","disgust","fear","happy","neutral","ps","sad","surprise"]
files, y = [], []

base_dir = "/content/TESS/TESS Toronto emotional speech set data"
for root, _, fns in os.walk(base_dir):
    for f in fns:
        if f.endswith(".wav"):
            folder_name = os.path.basename(root).lower()
            for idx, emotion in enumerate(emotion_labels):
                if emotion in folder_name:
                    files.append(os.path.join(root, f))
                    y.append(idx)
                    break

print("‚úÖ Total audio files found:", len(files))

# Use only 1400 samples for fast training
files, y = files[:1400], y[:1400]
print("‚úÖ Using subset:", len(files))

# ===========================================
# 7Ô∏è‚É£ CREATE DATASET
# ===========================================
dataset = Dataset.from_dict({"path": files, "label": y})
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base")

# ===========================================
# 8Ô∏è‚É£ PREPROCESS FUNCTION
# ===========================================
def preprocess(batch):
    speech, sr = torchaudio.load(batch["path"])
    speech = torchaudio.transforms.Resample(sr, 16000)(speech)
    inputs = processor(
        speech.squeeze().numpy(),
        sampling_rate=16000,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=16000 * 4
    )
    return {"input_values": inputs.input_values[0], "labels": torch.tensor(batch["label"])}

dataset = dataset.map(preprocess)

# ===========================================
# 9Ô∏è‚É£ TRAIN / TEST SPLIT
# ===========================================
split = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset, eval_dataset = split["train"], split["test"]

def collate_fn(batch):
    input_values = [x["input_values"] for x in batch]
    labels = [x["labels"] for x in batch]
    batch_inputs = processor.pad({"input_values": input_values}, padding=True, return_tensors="pt")
    batch_inputs["labels"] = torch.tensor(labels)
    return batch_inputs

# ===========================================
# üîü LOAD MODEL
# ===========================================
from transformers import Wav2Vec2ForSequenceClassification

model = Wav2Vec2ForSequenceClassification.from_pretrained(
    "facebook/wav2vec2-base",
    num_labels=len(emotion_labels),
    label2id={l:i for i,l in enumerate(emotion_labels)},
    id2label={i:l for i,l in enumerate(emotion_labels)}
)

# 1Ô∏è‚É£ Freeze all encoder parameters first
for param in model.wav2vec2.parameters():
    param.requires_grad = False

# 2Ô∏è‚É£ Unfreeze the last two encoder layers (fine-tune only these)
for name, param in model.wav2vec2.named_parameters():
    if "encoder.layers.10" in name or "encoder.layers.11" in name:
        param.requires_grad = True

print("‚úÖ Trainable params:", sum(p.numel() for p in model.parameters() if p.requires_grad))

# ===========================================
# 1Ô∏è‚É£1Ô∏è‚É£ TRAINING ARGUMENTS
# ===========================================
training_args = TrainingArguments(
    output_dir="./wav2vec2-tess",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=10,    # instead of 3
    learning_rate=1e-4,     # slightly lower for stability
    fp16=True,
    logging_dir="./logs",
    save_total_limit=1,
    save_strategy="epoch"
)

# ===========================================
# 1Ô∏è‚É£2Ô∏è‚É£ TRAINER SETUP
# ===========================================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=collate_fn
)

# ===========================================
# 1Ô∏è‚É£3Ô∏è‚É£ TRAIN MODEL
# ===========================================
trainer.train()
metrics = trainer.evaluate()
print("üìä Evaluation metrics:", metrics)

# ===========================================
# 1Ô∏è‚É£4Ô∏è‚É£ CONFUSION MATRIX
# ===========================================
preds_output = trainer.predict(eval_dataset)
y_true = preds_output.label_ids
y_pred = np.argmax(preds_output.predictions, axis=1)

acc = accuracy_score(y_true, y_pred)
print(f"‚úÖ Accuracy: {acc*100:.2f}%")

from sklearn.utils.multiclass import unique_labels

# Filter labels that actually appear
used_labels = unique_labels(y_true, y_pred)
used_emotions = [emotion_labels[i] for i in used_labels]

cm = confusion_matrix(y_true, y_pred, labels=used_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=used_emotions)
fig, ax = plt.subplots(figsize=(8,6))
disp.plot(ax=ax, cmap="Blues", xticks_rotation=45)
plt.title("Speech Emotion Classification - Confusion Matrix")
plt.show()

# ===========================================
# 1Ô∏è‚É£5Ô∏è‚É£ SAVE & DOWNLOAD MODEL
# ===========================================
save_dir = "/content/fine_tuned_wav2vec2_tess"
model.save_pretrained(save_dir)
processor.save_pretrained(save_dir)

# Zip and prepare for download
!zip -r fine_tuned_wav2vec2_tess.zip $save_dir
from google.colab import files
files.download("fine_tuned_wav2vec2_tess.zip")

# ===========================================
# 1Ô∏è‚É£6Ô∏è‚É£ DEMO PREDICTION
# ===========================================
import random

path = random.choice(files)
speech, sr = torchaudio.load(path)
speech = torchaudio.transforms.Resample(sr, 16000)(speech)
inputs = processor(speech.squeeze().numpy(), sampling_rate=16000, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits
pred = torch.argmax(logits, dim=-1).item()

print("üéß File:", path)
print("üéØ Predicted Emotion:", emotion_labels[pred])

